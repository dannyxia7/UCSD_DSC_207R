{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8i3Bx6yjeZi"
      },
      "source": [
        "# Week 9 Study Notebook\n",
        "\n",
        "# UC San Diego OMDS DSC 207"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeBnd-nmjeZk"
      },
      "source": [
        "## Assignment Instructions-\n",
        "1. You may use any visualization tool - seaborn/matplotlib for a task\n",
        "2. Sample outputs are provided but your output may vary based on choice of library and parameters.\n",
        "3. Use the mandatory parameter values wherever specified"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Currently, there is an autograder configured for the week 9 assignment on Gradescope. Please feel free to ignore this as we will be grading the entire assignment manually."
      ],
      "metadata": {
        "id": "k6EBncn8liAW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgLrIkqOjeZm"
      },
      "source": [
        "# Natural Language Processing with `nltk`\n",
        "\n",
        "`nltk` is the most popular Python package for Natural Language processing, it provides algorithms for importing, cleaning, pre-processing text data in human language and then apply computational linguistics algorithms like sentiment analysis.\n",
        "\n",
        "It also includes many easy-to-use datasets in the `nltk.corpus` package, we can download for example the `movie_reviews` package using the `nltk.download` function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2DIQ43GjeZn"
      },
      "source": [
        "## Inspect the Movie Reviews Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1vWI9iTjeZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a29c1a-2c0a-41f7-ccda-92c7c9d2187e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "# Uncomment the below line and run this cell if you need to install nltk\n",
        "#!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9xqS4pXEjeZn"
      },
      "outputs": [],
      "source": [
        "#Run this cell for all the imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the sentiment intensity analyzer\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "UkSnjr3NAtxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717c3f97-7bba-47a2-fb8e-06a105337773"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OPE2kTE0jeZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79cb88d7-743c-4cf8-e2d8-2bf34761c788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#Run this cell to download the dataset\n",
        "nltk.download(\"movie_reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_TYgNRmjeZo"
      },
      "source": [
        "You can also list and download other datasets interactively, just type:\n",
        "\n",
        "`nltk.download()`\n",
        "    \n",
        "in the Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cN6fiwBxjeZo"
      },
      "outputs": [],
      "source": [
        "#Run this cell to import the dataset\n",
        "from nltk.corpus import movie_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lFR08MDtjeZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11ab553-7800-4f0f-de5f-fea89ba19883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#Run this cell for later use in tokenization\n",
        "nltk.download('vader_lexicon')  # for sentiment analysis\n",
        "nltk.download('punkt_tab')  # for tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7MRewpjeZo"
      },
      "source": [
        "## 1. Tokenize Text in Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3LZbdx56jeZo"
      },
      "outputs": [],
      "source": [
        "#Run this cell\n",
        "romeo_text = \"\"\"Why then, O brawling love! O loving hate!\n",
        "O any thing, of nothing first create!\n",
        "O heavy lightness, serious vanity,\n",
        "Misshapen chaos of well-seeming forms,\n",
        "Feather of lead, bright smoke, cold fire, sick health,\n",
        "Still-waking sleep, that is not what it is!\n",
        "This love feel I, that feel no love in this.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NOYVwETgjeZo"
      },
      "source": [
        "The first step in Natural Language processing is generally to split the text into words, this process might appear simple but it is very tedious to handle all corner cases, see for example all the issues with punctuation we have to solve if we just start with a split on whitespace.\n",
        "\n",
        "1.1 **Split `romeo_text` by spaces and store the resultant list of words in the variable `romeo_tokens`** [0.5 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": [],
        "id": "-5gZFidFjeZo"
      },
      "outputs": [],
      "source": [
        "romeo_tokens = romeo_text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-UAgp_gBjeZq"
      },
      "outputs": [],
      "source": [
        "assert type(romeo_tokens) == list\n",
        "assert len(romeo_tokens) == 52"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2T0E5MRLjeZq"
      },
      "source": [
        "`nltk` has a sophisticated word tokenizer trained on English named `punkt` which we imported earlier in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "IzelcAqQjeZq"
      },
      "source": [
        "1.2  **Use the `nltk.word_tokenize(text)` function to properly tokenize `romeo_text` and stores the result as `romeo_words`. Print the resultant list.** Compare it to the whitespace splitting we used above and mention the difference. [0.5 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "tags": [],
        "id": "ww0hQiEljeZq"
      },
      "outputs": [],
      "source": [
        "romeo_words = nltk.word_tokenize(romeo_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b8uUSX-HjeZq"
      },
      "outputs": [],
      "source": [
        "assert type(romeo_words) == list\n",
        "assert len(romeo_words) == 68"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "jJ7L2t6sjeZq"
      },
      "source": [
        "## 2. Build a bag-of-words model\n",
        "\n",
        "The simplest model for analyzing text is just to think about text as an unordered collection of words (bag-of-words). This can generally allow to infer from the text the category, the topic or the sentiment.\n",
        "\n",
        "From the bag-of-words model we can build features to be used by a classifier, here we assume that each word is a feature that can either be `True` or `False`.\n",
        "We implement this in Python as a dictionary where for each word in a sentence we associate `True`.\n",
        "\n",
        "2.1 **Write a function `build_bag_of_words(words)` that returns such a dictionary with {word : True} format given a set of words. Call the function with `romeo_words` and store the resultant dictionary as `romeo_word_dict`.** [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "tags": [],
        "id": "d3xHS-l5jeZq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_bag_of_words_features(words):\n",
        "    word_dict = {word:True for word in words}\n",
        "    return word_dict\n",
        "\n",
        "\n",
        "romeo_word_dict = build_bag_of_words_features(romeo_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xUqOjgjDjeZq"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "assert type(build_bag_of_words_features(romeo_words)) == dict\n",
        "assert sum(value for value in romeo_word_dict.values() if value) == 45"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfjjBI50jeZq"
      },
      "source": [
        "This is what we wanted, but we notice that also punctuation like \"!\" and words useless for classification purposes like \"of\" or \"that\" are also included.\n",
        "Those words are named \"stopwords\" and `nltk` has a convenient corpus we can download:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "aOQS0TKQjeZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f294de9-981e-45fe-c92d-5d3ba87ee662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "#Run this cell\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "SNXMb6AnjeZr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06eeb53e-f8ba-497f-9592-657c11b70b08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#Run this cell\n",
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_lPMqep_jeZr"
      },
      "source": [
        "Using the Python `string.punctuation` list and the English stopwords we can build better features by filtering out those words that would not help in the classification.\n",
        "\n",
        "2.2 **Create a list `useless_words` that is a collection of stopwords in english and the punctuation characters.** [0.5 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "tags": [],
        "id": "Zpq6sCA9jeZr"
      },
      "outputs": [],
      "source": [
        "useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Yt5CQ8amjeZr"
      },
      "outputs": [],
      "source": [
        "assert type(useless_words) == list\n",
        "assert len(useless_words) == 211"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "oro_XnIJjeZr"
      },
      "source": [
        "2.3 **Write a function `build_bag_of_words_features_filtered(words)` that returns a filtered bag of words - a dictionary with only useful words as key and 1 as the value. Call this function with `romeo_words` and store the resultant dictionary as `romeo_useful_word_dict`.** [1 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "tags": [],
        "id": "6m4Ob8mXjeZr"
      },
      "outputs": [],
      "source": [
        "def build_bag_of_words_features_filtered(words):\n",
        "    return {word:1 for word in words if not word in useless_words}\n",
        "romeo_useful_word_dict = build_bag_of_words_features_filtered(romeo_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "PaJxLlwMjeZr"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "assert type(build_bag_of_words_features_filtered(romeo_words)) == dict\n",
        "assert len(romeo_useful_word_dict) == 31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Jgs6tClFjeZr"
      },
      "source": [
        "## 3. Frequencies of Words\n",
        "\n",
        "It is common to explore a dataset before starting the analysis, in this section we will find the most common words and plot their frequency.\n",
        "\n",
        "3.1. Using the `movie_reviews.words()` (the nltk corpus we imported previously) with no argument we can extract the words from the entire dataset as `all_words` and check that it is about 1.6 millions. [0.5 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "tags": [],
        "id": "OP--a4nqjeZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72324b4-a4bc-4afa-ba6d-22d6109857de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1583820"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "all_words = movie_reviews.words()\n",
        "len(all_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "92VCYW3ijeZr"
      },
      "source": [
        "3.2. Filter out `useless_words` as defined in the previous section, and create a new list `filtered_words` this will reduce the length of the dataset by more than a factor of 2. (Hint - Use python list comprehension) [0.5 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "tags": [],
        "id": "-vMuUmT_jeZr"
      },
      "outputs": [],
      "source": [
        "filtered_words = [word for word in all_words if word not in useless_words]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llssooadsehd",
        "outputId": "d10cf8f4-5c58-43e3-c512-c4d9e96ea07f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "710579"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8OjO8MQ4jeZr"
      },
      "outputs": [],
      "source": [
        "assert type(filtered_words) == list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ9JvlqdjeZw"
      },
      "source": [
        "The `collection` package of the standard library contains a `Counter` class that is handy for counting frequencies of words in our list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Y8Oy7kKbjeZw"
      },
      "outputs": [],
      "source": [
        "# Run this cell\n",
        "from collections import Counter\n",
        "word_counter = Counter(filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f47yTmyMjeZw"
      },
      "source": [
        "3.3 It also has a [most_common() ](https://pythontic.com/containers/counter/most_common) method of the word_counter and store the top 10 used words from the corpus in `most_common_words`. [0.5 pt]?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "tags": [],
        "id": "7YS84BJtjeZw"
      },
      "outputs": [],
      "source": [
        "most_common_words = word_counter.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(most_common_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJYEwxZtsnnh",
        "outputId": "7d607e46-6e54-4866-c2e5-babf7df6abb3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('film', 9517), ('one', 5852), ('movie', 5771), ('like', 3690), ('even', 2565), ('good', 2411), ('time', 2411), ('story', 2169), ('would', 2109), ('much', 2049)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6pkTYJeFjeZw"
      },
      "outputs": [],
      "source": [
        "assert type(most_common_words) == list\n",
        "assert len(most_common_words) == 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "kCLf4jiFjeZw"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## 4. Sentiment Analysis [2 pt]\n",
        "\n",
        "Using the sentiment intensity analyzer, loop over the `list_sentences` and print the polarity scores of each of the sentence. (Hint - refer to lecture notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "tags": [],
        "id": "xY8I4gNIjeZw"
      },
      "outputs": [],
      "source": [
        "\n",
        "list_sentences = [\"Hello, how are you?\", \"Today is a nice day\", \"I don't like the food at the cafe\", \"This is the worst pizza I have ever had.\", \"The orange juice is delicious!\", \"I am late to class.\" ]\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "for text in list_sentences:\n",
        "  scores = analyzer.polarity_scores(text)\n",
        "  compound_score = scores['compound']\n",
        "\n",
        "  print(f'Text: {text}.\\n, Scores: {scores}')"
      ],
      "metadata": {
        "id": "rENJdd-pzLxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f324a1dd-1767-420f-c7dd-afe1178426e3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Hello, how are you?.\n",
            ", Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Text: Today is a nice day.\n",
            ", Scores: {'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.4215}\n",
            "Text: I don't like the food at the cafe.\n",
            ", Scores: {'neg': 0.232, 'neu': 0.768, 'pos': 0.0, 'compound': -0.2755}\n",
            "Text: This is the worst pizza I have ever had..\n",
            ", Scores: {'neg': 0.339, 'neu': 0.661, 'pos': 0.0, 'compound': -0.6249}\n",
            "Text: The orange juice is delicious!.\n",
            ", Scores: {'neg': 0.0, 'neu': 0.501, 'pos': 0.499, 'compound': 0.6114}\n",
            "Text: I am late to class..\n",
            ", Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RkXkhjDQ2ehS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ta_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "otter": {
      "OK_FORMAT": true,
      "tests": {
        "q1": {
          "name": "q1",
          "points": 0.5,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q2": {
          "name": "q2",
          "points": 0.5,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q3": {
          "name": "q3",
          "points": 1,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q4": {
          "name": "q4",
          "points": 0.5,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q5": {
          "name": "q5",
          "points": 1,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q6": {
          "name": "q6",
          "points": 0.5,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q7": {
          "name": "q7",
          "points": 0.5,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q8": {
          "name": "q8",
          "points": 0.5,
          "suites": [
            {
              "cases": [],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        }
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}